# -*- coding: utf-8 -*-
"""autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Q3tn0qJAc5bh7UDvV_yfnvuGl8ztGDe
"""

import os
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
import joblib

# ---------- CONFIG ----------
DATA_PATH = "/content/enhanced data.csv"
OUTPUT_CSV = "/content/enhanced_risk_scores.csv"
MODEL_PATH = "/content/mlp_autoencoder.joblib"
SCALER_PATH = "/content/feature_scaler.save"
RANDOM_STATE = 57
SAMPLE_MAX_ROWS = 20000        # reduce for speed if needed
TEST_SIZE = 0.2
TOP_K_CAT = 20                # keep top K device types
MLP_HIDDEN = (32,)            # hidden structure
MLP_MAX_ITER = 50             # max iterations
ANOMALY_PCTILE_CAP = 95      # used to cap anomaly normalization
# ----------------------------

# ---------- Load data ----------
df = pd.read_csv(DATA_PATH, low_memory=False)
print("Loaded rows:", len(df))

if len(df) > SAMPLE_MAX_ROWS:
    df = df.sample(n=SAMPLE_MAX_ROWS, random_state=RANDOM_STATE).reset_index(drop=True)
    print("Sampled to", len(df), "rows for faster run.")

# ---------- Ensure columns exist (defaults if missing) ----------
required_cols = ['login_time','session_minutes','device_type','files_accessed',
                 'bytes_downloaded','sensitive_command','failed_logins','command_count']
for c in required_cols:
    if c not in df.columns:
        #fallback
        if c in ['sensitive_command']:
            df[c] = 0   # assume 0=not sensitive
        else:
            df[c] = 0.0

# Extracting day, hour and weekend from timestamp
df['login_time'] = pd.to_datetime(df['login_time'], errors='coerce')
df['day'] = df['login_time'].dt.date
df['hour'] = df['login_time'].dt.hour
df['weekday'] = df['login_time'].dt.weekday  # 0=Mon ... 6=Sun

# Off-hours flag: before 08:00 or after 18:00 or weekend
df['login_risk'] = ((df['hour'] < 8) | (df['hour'] > 18) | (df['weekday'] >= 5)).astype(int)

# Device risk: unknown/uncommon devices get higher base risk later (we'll encode)
device_counts = df['device_type'].value_counts(normalize=True)
df['device_risk'] = df['device_type'].map(device_counts).fillna(0.0)

# ---------- Feature selection for autoencoder ----------#

# We'll use a mix of numeric features and one-hot encoded categorical device_type
numeric_cols = ['session_minutes', 'files_accessed', 'bytes_downloaded',
                'failed_logins', 'command_count']  # primary numeric behaviour features
# Ensure numeric types
for nc in numeric_cols:
    df[nc] = pd.to_numeric(df[nc], errors='coerce').fillna(0.0)

# Categorical device_type: keep top-K categories, rest -> "__OTHER__"
device_col = 'device_type'
top_devices = df[device_col].value_counts().nlargest(TOP_K_CAT).index.tolist()
df['_device_reduced'] = df[device_col].where(df[device_col].isin(top_devices), other="__OTHER__").astype(str).fillna("__MISSING__")

# Build numeric matrix: numerics + sensitive_command + login_risk (as input to autoencoder)
# We include login_risk and sensitive_command as input features too so model learns context
base_numeric = df[numeric_cols].values
extra_numeric = df[['sensitive_command','login_risk']].values

# One-hot encode reduced device_type
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
device_ohe = ohe.fit_transform(df[['_device_reduced']])

# Concatenate features
X_raw = np.hstack([base_numeric, extra_numeric, device_ohe])
print("Feature matrix shape for autoencoder:", X_raw.shape)

# ---------- Impute + Scale ----------
imp = SimpleImputer(strategy='mean')
X_imp = imp.fit_transform(X_raw)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)

# Save imputers/scalers for later use
joblib.dump(imp, "/content/autoencoder_imputer.joblib")
joblib.dump(scaler, SCALER_PATH)
joblib.dump(ohe, "/content/device_ohe.joblib")

# ---------- Train/test split ----------
X_train, X_test = train_test_split(X_scaled, test_size=TEST_SIZE, random_state=RANDOM_STATE)
print("Train/test shapes:", X_train.shape, X_test.shape)

# ---------- Train a scikit-learn MLP autoencoder ----------
mlp = MLPRegressor(hidden_layer_sizes=MLP_HIDDEN,
                   activation='relu',
                   solver='adam',
                   batch_size=256,
                   max_iter=MLP_MAX_ITER,
                   random_state=RANDOM_STATE,
                   verbose=False)

print("Training MLP autoencoder (this may take a while depending on MLP_MAX_ITER)...")
mlp.fit(X_train, X_train)
joblib.dump(mlp, MODEL_PATH)
print("Saved autoencoder model to", MODEL_PATH)

# ---------- Compute reconstruction errors (anomaly scores) ----------
recon_test = mlp.predict(X_test)
mse_test = np.mean((X_test - recon_test)**2, axis=1)

# Compute threshold: max(mean+3*std, 95th percentile)
mean_mse = mse_test.mean()
std_mse = mse_test.std()
pct95 = np.percentile(mse_test, 95)
threshold = max(mean_mse + 3*std_mse, pct95)
print(f"Anomaly threshold determined: {threshold:.6f}")

# Attach anomaly score back to corresponding rows: we will create scores for *all* rows by predicting on X_scaled
recon_all = mlp.predict(X_scaled)
mse_all = np.mean((X_scaled - recon_all)**2, axis=1)
df['anomaly_score'] = mse_all
df['anomaly_flag'] = (df['anomaly_score'] > threshold).astype(int)

# ---------- Build revised risk score combining additional attributes ----------
# Normalize anomaly score to [0,1] (cap at ANOMALY_PCTILE_CAP percentile to reduce outlier impact)
cap_val = np.percentile(df['anomaly_score'], ANOMALY_PCTILE_CAP)
A_max = max(cap_val, 1e-9) * 2.0      # small guard to avoid zero
df['anomaly_norm'] = np.minimum(df['anomaly_score'], A_max) / A_max

# Prepare behavioural features scaled to 0-1 (MinMax per full dataset)
behaviour_cols = numeric_cols + ['failed_logins','command_count']  # numeric_cols already includes some
# make sure columns exist
for c in behaviour_cols:
    if c not in df.columns:
        df[c] = 0.0

scaler_beh = MinMaxScaler()
df[behaviour_cols] = scaler_beh.fit_transform(df[behaviour_cols])
joblib.dump(scaler_beh, "/content/behaviour_minmax_scaler.joblib")

# Device risk score: give uncommon devices slightly higher risk.
# We'll compute device_popularity and map to device_risk in [0,1] where less-popular -> higher risk
device_counts = df[device_col].value_counts(normalize=True)
df['device_popularity'] = df[device_col].map(device_counts).fillna(0.0)
# device_risk: invert popularity
df['device_risk'] = 1.0 - df['device_popularity']
# normalize device_risk to 0-1
if df['device_risk'].max() > 0:
    df['device_risk'] = df['device_risk'] / df['device_risk'].max()
else:
    df['device_risk'] = 0.0

# Day-of-week risk: assign higher base risk for weekend logins (example)
df['day_risk'] = df['weekday'].apply(lambda x: 0.5 if x >=5 else 0.0)  # 0.5 for weekend, 0 for weekday
# You may refine this with holiday calendar lookup if available.

# Sensitive command is binary: ensure in 0..1
df['sensitive_command'] = df['sensitive_command'].clip(0,1)

# Compose final weighted risk score: define weights (tweakable)
weights = {
    'anomaly_norm'    : 0.30,   # normalized anomaly magnitude
    'anomaly_flag'    : 0.15,   # whether above threshold
    'login_risk'      : 0.08,   # off-hours/login-time risk
    'day_risk'        : 0.04,   # weekend/holiday baseline
    'session_minutes' : 0.08,
    'files_accessed'  : 0.07,
    'bytes_downloaded': 0.05,
    'failed_logins'   : 0.10,
    'command_count'   : 0.03,
    'sensitive_command':0.06,
    'device_risk'     : 0.04
}
# Ensure weights sum roughly to 1.0 (they sum to 1.0 above)
total_w = sum(weights.values())
if abs(total_w - 1.0) > 1e-6:
    # normalize weights to sum to 1
    weights = {k: v/total_w for k,v in weights.items()}

# Build the score
df['risk_base'] = (
      df['anomaly_norm'] * weights['anomaly_norm']
    + df['anomaly_flag'] * weights['anomaly_flag']
    + df['login_risk'] * weights['login_risk']
    + df['day_risk'] * weights['day_risk']
    + df['session_minutes'] * weights['session_minutes']
    + df['files_accessed'] * weights['files_accessed']
    + df['bytes_downloaded'] * weights['bytes_downloaded']
    + df['failed_logins'] * weights['failed_logins']
    + df['command_count'] * weights['command_count']
    + df['sensitive_command'] * weights['sensitive_command']
    + df['device_risk'] * weights['device_risk']
)

# scale to 0-100 and clip
df['risk_score'] = (df['risk_base'] * 100).clip(0,100)

# Rule-based boosts / multipliers
# If anomaly flagged + sensitive command + off-hours -> strong boost
mask_boost = (df['anomaly_flag']==1) & (df['sensitive_command']==1) & (df['login_risk']==1)
df.loc[mask_boost, 'risk_score'] = (df.loc[mask_boost, 'risk_score'] * 1.25).clip(0,100)

# If failed_logins very high (top 1% within dataset), further boost
failed_thresh = np.percentile(df['failed_logins'], 99)
df.loc[df['failed_logins'] > failed_thresh, 'risk_score'] = (df.loc[df['failed_logins'] > failed_thresh, 'risk_score'] * 1.2).clip(0,100)

# ---------- Per-day min-max normalization (relative score) ----------
df['date'] = df['login_time'].dt.date
def per_day_minmax(group):
    minv = group['risk_score'].min()
    maxv = group['risk_score'].max()
    if maxv > minv:
        group['risk_score_norm_day'] = ((group['risk_score'] - minv) / (maxv - minv)) * 100.0
    else:
        group['risk_score_norm_day'] = 0.0
    return group

df = df.groupby('date', group_keys=False).apply(per_day_minmax)
df['risk_score_norm_day'] = df['risk_score_norm_day'].round(2)

# ---------- Save artifacts ----------
# Save risk scores CSV (include many helpful columns)
out_cols = ['user_id', 'login_time','day','hour','weekday','device_type',
            'anomaly_score','anomaly_flag','anomaly_norm',
            'session_minutes','files_accessed','bytes_downloaded','failed_logins','command_count',
            'sensitive_command','device_risk','login_risk','risk_score','risk_score_norm_day']
# create output directory if not present
out_dir = os.path.dirname(OUTPUT_CSV)
if out_dir and not os.path.exists(out_dir):
    os.makedirs(out_dir, exist_ok=True)

df.to_csv(OUTPUT_CSV, index=False, columns=[c for c in out_cols if c in df.columns])
print("Saved revised risk scores to:", OUTPUT_CSV)

# Save model and encoders/scalers (already saved implicitly above)
# (scaler saved to SCALER_PATH earlier, mlp saved to MODEL_PATH)

# Quick summary
print("Summary:")
print(" - rows:", len(df))
print(" - anomaly_score: mean {:.4f}, std {:.4f}, threshold {:.6f}".format(df['anomaly_score'].mean(), df['anomaly_score'].std(), threshold))
print(" - risk_score: min {:.2f}, 25% {:.2f}, median {:.2f}, 75% {:.2f}, max {:.2f}".format(
    df['risk_score'].min(), df['risk_score'].quantile(0.25), df['risk_score'].median(), df['risk_score'].quantile(0.75), df['risk_score'].max()))

import pandas as pd
import matplotlib.pyplot as plt

# Load data with risk scores
df = pd.read_csv("enhanced_risk_scores.csv", parse_dates=['login_time'])
user_column = 'user_id'

# Pick the top 10 users by mean risk_score
top_users = (
    df.groupby(user_column)['risk_score']
    .mean()
    .nlargest(10)
    .index
)

# Filter only those users
df_top = df[df[user_column].isin(top_users)].copy()

# Sort by login_time
df_top.sort_values('login_time', inplace=True)

# Plot timeline per user
plt.figure(figsize=(14, 8))

for i, user in enumerate(top_users):
    user_data = df_top[df_top[user_column] == user]
    # shift each user vertically for clarit
    y_values = [i]*len(user_data)
    plt.scatter(
        user_data['login_time'],
        y_values,
        c=user_data['risk_score'],  # color by risk score
        cmap='Reds',
        s=50,
        label=str(user) if i==0 else None
    )

plt.yticks(range(len(top_users)), top_users)
plt.colorbar(label='Risk Score')
plt.xlabel('Time')
plt.ylabel('User')
plt.title('Timeline of Actions for Top 10 Users by Risk Score')
plt.tight_layout()
plt.show()